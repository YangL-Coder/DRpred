{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79319f84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of individual locations:\n",
      "Exosome: 28304\n",
      "Nucleus: 19301\n",
      "Nucleoplasm: 12807\n",
      "Chromatin: 12893\n",
      "Cytosol: 14686\n",
      "Ribosome: 7796\n",
      "Cytoplasm: 3597\n",
      "Nucleolus: 10000\n",
      "Membrane: 6047\n",
      "\n",
      "Counts of co-occurrence for each pair of locations:\n",
      "('Exosome', 'Ribosome'): 15564\n",
      "('Exosome', 'Membrane'): 12068\n",
      "('Exosome', 'Nucleolus'): 19974\n",
      "('Exosome', 'Nucleus'): 32336\n",
      "('Cytosol', 'Exosome'): 29204\n",
      "('Cytoplasm', 'Exosome'): 3594\n",
      "('Exosome', 'Nucleoplasm'): 25548\n",
      "('Chromatin', 'Exosome'): 25712\n",
      "('Membrane', 'Ribosome'): 4328\n",
      "('Nucleolus', 'Ribosome'): 8282\n",
      "('Nucleus', 'Ribosome'): 11752\n",
      "('Cytosol', 'Ribosome'): 11440\n",
      "('Cytoplasm', 'Ribosome'): 1082\n",
      "('Nucleoplasm', 'Ribosome'): 10210\n",
      "('Chromatin', 'Ribosome'): 10188\n",
      "('Membrane', 'Nucleolus'): 7410\n",
      "('Membrane', 'Nucleus'): 10742\n",
      "('Cytosol', 'Membrane'): 10616\n",
      "('Cytoplasm', 'Membrane'): 974\n",
      "('Membrane', 'Nucleoplasm'): 9422\n",
      "('Chromatin', 'Membrane'): 9304\n",
      "('Nucleolus', 'Nucleus'): 17016\n",
      "('Cytosol', 'Nucleolus'): 15674\n",
      "('Cytoplasm', 'Nucleolus'): 1386\n",
      "('Nucleolus', 'Nucleoplasm'): 16294\n",
      "('Chromatin', 'Nucleolus'): 16736\n",
      "('Cytosol', 'Nucleus'): 23188\n",
      "('Cytoplasm', 'Nucleus'): 3614\n",
      "('Nucleoplasm', 'Nucleus'): 21504\n",
      "('Chromatin', 'Nucleus'): 21588\n",
      "('Cytoplasm', 'Cytosol'): 2106\n",
      "('Cytosol', 'Nucleoplasm'): 19816\n",
      "('Chromatin', 'Cytosol'): 19790\n",
      "('Cytoplasm', 'Nucleoplasm'): 1834\n",
      "('Chromatin', 'Cytoplasm'): 1814\n",
      "('Chromatin', 'Nucleoplasm'): 20666\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "# Define the file path\n",
    "file = '../dataset/training_validation_annotation.txt'\n",
    "\n",
    "# Function to read file content and return it as a list of lines\n",
    "def read_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "# Read content from the file\n",
    "data = read_file(file)\n",
    "\n",
    "# Initialize dictionaries to count occurrences\n",
    "location_count = defaultdict(int)\n",
    "pair_count = defaultdict(int)\n",
    "\n",
    "# Process each line in the data\n",
    "for line in data:\n",
    "    locations = line.strip().split('|')\n",
    "    # Count occurrences of each individual location\n",
    "    for location in locations:\n",
    "        location_count[location] += 1\n",
    "    # Count occurrences of each pair of locations\n",
    "    for pair in combinations(locations, 2):\n",
    "        pair_count[tuple(sorted(pair))] += 1\n",
    "\n",
    "# Find all unique locations\n",
    "all_locations = set(location_count.keys())\n",
    "\n",
    "# Calculate total co-occurrence count for each pair of locations\n",
    "total_pair_count = defaultdict(int)\n",
    "for loc1 in all_locations:\n",
    "    for loc2 in all_locations:\n",
    "        if loc1 != loc2:\n",
    "            pair = tuple(sorted((loc1, loc2)))\n",
    "            total_pair_count[pair] += pair_count[pair]\n",
    "\n",
    "# Output the results\n",
    "print(\"Counts of individual locations:\")\n",
    "for location, count in location_count.items():\n",
    "    print(f\"{location}: {count}\")\n",
    "\n",
    "print(\"\\nCounts of co-occurrence for each pair of locations:\")\n",
    "for pair, count in total_pair_count.items():\n",
    "    print(f\"{pair}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4475c14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# node name\n",
    "nodes = [\"Exosome\", \"Nucleus\", \"Nucleoplasm\", \"Chromatin\", \"Cytoplasm\", \n",
    "         \"Nucleolus\", \"Cytosol\", \"Membrane\", \"Ribosome\"]\n",
    "\n",
    "\n",
    "cooccurrence_matrix = np.array([\n",
    "    [28304, 32336, 25548, 25712, 3594, 19974, 29204, 12068, 15564],\n",
    "    [32336, 19301, 21504, 21588, 3614, 17016, 23188, 10742, 11752],\n",
    "    [25548, 21504, 12807, 20666, 1834, 16294, 19816, 9422,  10210],\n",
    "    [25712, 21588, 20666, 12893, 1814, 16736, 19790, 9304,  10188],\n",
    "    [3594,  3614,  1834,  1814,  14686, 1836, 2106,  974,   1082],\n",
    "    [19974, 17016, 16294, 16736, 1836,  7796, 15674, 7410,  8282],\n",
    "    [29204, 23188, 19816, 19790, 2106, 15674, 3597,  10616, 11440],\n",
    "    [12068, 10742, 9422,  9304,  974,  7410,  10616, 10000,  4328],\n",
    "    [15564, 11752, 10210, 10188, 1082, 8282,  11440, 4328,  6047]\n",
    "])\n",
    "\n",
    "\n",
    "# set the threshold\n",
    "threshold = 0\n",
    "\n",
    "# Create an undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "G.add_nodes_from(nodes)\n",
    "\n",
    "\n",
    "edge_labels = {}\n",
    "for i in range(len(nodes)):\n",
    "    for j in range(i + 1, len(nodes)):\n",
    "        if cooccurrence_matrix[i, j] > threshold:\n",
    "            G.add_edge(nodes[i], nodes[j])\n",
    "            edge_labels[(nodes[i], nodes[j])] = f\"{cooccurrence_matrix[i, j]:.4f}\"\n",
    "\n",
    "# Convert an undirected graph to a directed graph to build a Bayesian network\n",
    "bn = BayesianNetwork()\n",
    "\n",
    "for edge in G.edges():\n",
    "    bn.add_edge(edge[0], edge[1])\n",
    "\n",
    "\n",
    "\n",
    "# load the training data\n",
    "training_validation_annotation_encoded_path = 'D:/RNA/DRpred/dataset/training_validation_annotation_encoded.csv'\n",
    "training_data = pd.read_csv(training_validation_annotation_encoded_path)\n",
    "\n",
    "# Train a Bayesian network using maximum likelihood estimation\n",
    "bn.fit(training_data, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "# print trained CPD (Conditional Probability distribution)\n",
    "print(\"Conditional Probability Distributions:\")\n",
    "for cpd in bn.get_cpds():\n",
    "    print(cpd)\n",
    "\n",
    "# Inference with variable elimination\n",
    "infer = VariableElimination(bn)\n",
    "\n",
    "# compute marginal probabilities\n",
    "marginal_probabilities = {}\n",
    "for node in nodes:\n",
    "    marginal_prob = infer.query([node], show_progress=False)\n",
    "    marginal_probabilities[node] = marginal_prob.values[1]  # 获取P(node=1)\n",
    "# Normalized marginal probability\n",
    "total_marginal_probability = sum(marginal_probabilities.values())\n",
    "for node in nodes:\n",
    "    marginal_probabilities[node] /= total_marginal_probability\n",
    "\n",
    "for node, prob in marginal_probabilities.items():\n",
    "    print(f\"{node}: {prob:.3f}\")\n",
    "\n",
    "\n",
    "for i in range(len(nodes)):\n",
    "    for j in range(i + 1, len(nodes)):\n",
    "        joint_prob = infer.query([nodes[i], nodes[j]], show_progress=False)\n",
    "        cooccurrence_probabilities[(nodes[i], nodes[j])] = joint_prob.values[1, 1]\n",
    "\n",
    "\n",
    "for edge, prob in cooccurrence_probabilities.items():\n",
    "    print(f\"{edge}: {prob:.4f}\")\n",
    "\n",
    "def extract_mean_prior_information(sequence_labels, cooccurrence_probabilities, nodes):\n",
    "    prior_information = []\n",
    "    for index, row in sequence_labels.iterrows():\n",
    "        \n",
    "        label_indices = [i for i, label in enumerate(row) if label == 1]\n",
    "        \n",
    "        # Compute the mean co-occurrence probability of the remaining irrelevant labels\n",
    "        remaining_indices = np.setdiff1d(range(len(nodes)), label_indices)\n",
    "        remaining_cooccurrence_probs = [cooccurrence_probabilities[(nodes[i], nodes[j])] for i in remaining_indices for j in remaining_indices if i < j]\n",
    "        if len(remaining_cooccurrence_probs) > 0:\n",
    "            mean_remaining_prob = np.mean(remaining_cooccurrence_probs)\n",
    "        else:\n",
    "            mean_remaining_prob = 0.0\n",
    "        \n",
    "        # Use the mean as the prior information value\n",
    "        prior_information.append(mean_remaining_prob)\n",
    "    \n",
    "    # Check if the number of lines of the prior matches the input file\n",
    "    if len(prior_information) != len(sequence_labels):\n",
    "        raise ValueError(\"Prior information rows do not match the number of rows in the input CSV file.\")\n",
    "    \n",
    "    return np.array(prior_information).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The label of each mRNA sequence was extracted\n",
    "sequence_labels = training_data  \n",
    "\n",
    "# calculate and print the prior\n",
    "prior_information = extract_mean_prior_information(sequence_labels, cooccurrence_probabilities, nodes)\n",
    "print(prior_information)\n",
    "\n",
    "# Convert the prior into a DataFrame\n",
    "prior_df = pd.DataFrame(prior_information, columns=[\"Prior Probability\"])\n",
    "\n",
    "#Save as a CSV file\n",
    "prior_csv_path = \"../dataset/train_prior_information.csv\"\n",
    "prior_df.to_csv(prior_csv_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be45ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bayesian_network(bn, edge_labels, cooccurrence_probabilities, filename='bayesian_network.png'):\n",
    "    G = nx.DiGraph()\n",
    "    for edge in bn.edges():\n",
    "        G.add_edge(edge[0], edge[1])  # Add one-way edges\n",
    "        G.add_edge(edge[1], edge[0])  # add reverse edges\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pos = nx.spring_layout(G)  \n",
    "    nx.draw(G, pos, with_labels=True, node_size=3000, node_color='skyblue', font_size=10, font_weight='bold', arrowsize=20, arrowstyle='-|>')\n",
    "    edge_labels_formatted = {(u, v): f\"{prob:.3f}\" for (u, v), prob in cooccurrence_probabilities.items()}\n",
    "    nx.draw_networkx_edge_labels(pos, edge_labels=edge_labels_formatted, font_color='red')\n",
    "    plt.title(\"Bayesian Network Structure with Co-occurrence Probabilities\")\n",
    "    plt.savefig(filename, dpi=300)  \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26285a1",
   "metadata": {},
   "source": [
    "indepence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc62779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Step 1: Load the feature vectors and prior information for the training and test sets\n",
    "test_features_path = \"../dataset/independent.csv\"\n",
    "train_prior_info_path = \"../dataset/train_prior_information.csv\"\n",
    "\n",
    "train_features = pd.read_csv(train_features_path)\n",
    "test_features = pd.read_csv(test_features_path)\n",
    "train_prior_info = pd.read_csv(train_prior_info_path)\n",
    "\n",
    "# Step 2: Train the Nearest neighbor model\n",
    "nbrs = NearestNeighbors(n_neighbors=9, algorithm='auto').fit(train_features)\n",
    "\n",
    "# Step 3: For each sample in the test set, find the index of the most similar training set sample\n",
    "distances, indices = nbrs.kneighbors(test_features)\n",
    "# Step 4: Extract the prior information on the most similar training set samples\n",
    "test_prior_info = [train_prior_info.iloc[idx[0]].values.tolist() for idx in indices]\n",
    "# Save the test set priors to a file\n",
    "test_prior_info_df = pd.DataFrame(test_prior_info)\n",
    "test_prior_info_df.to_csv(\"../dataset/test_prior_information.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f27cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
